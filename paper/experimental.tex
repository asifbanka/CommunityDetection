Our experimental setup can be roughly divided into five parts (see Figure~\ref{fig:pipeline}) but the 
respective parts differ slightly depending on the case whether we test overlapping or non-overlapping 
communities. First, we use the benchmark graph generator developed by Lacichinetti and Fortunato 
(LFR)~\cite{LFR08, LF09} which outputs graphs where the community information of each node 
is known. Then from each community in our input graph, we pick a fixed number of seed nodes, 
calculate their affinitites and feed it to our algorithm. Once the algorithm outputs the 
affinities, we classify the nodes into communitites and finally compare the output 
with the ground truth using normalized mutual information (NMI) as a metric~\cite{DDDA05}.


\paragraph{LFR.}
The LFR benchmark is a tool to generate random graphs with community structure that obey a power law distribution in both the node degrees and community sizes. The authors designed their code with the intention to establish a standard benchmark suite for community--detection algorithms. Using this benchmark they did a comparative analysis between a selection of algorithms~\cite{LF09}. For this reason, we chose this benchmark for our experiments and set the parameters in the same fashion. 

In the experiments with non--overlapping networks, the average degree of the networks is $20$, the maximum degree $50$, the minus exponent of the degree distribution is $2$, and that of the community size distribution is $1$. Further, we distinguish between \textit{small} and \textit{big} community sizes, which vary between $10$--$50$ and $20$--$100$ nodes respectively. We have two network sizes that we investigate: $1000$ and $5000$ nodes. We have one setup for every of the four combinations of network and community sizes. For every setup we create graphs with those parameters and a varying mixing factor $\mu$ (the percentage of edges that are leaving the communities) and compare the results from our algorithm with the ground truth given by the benchmark using NMI. Every point in the plot is the average over $100$ iterations using the same parameters.

For the overlapping networks, we do not vary the mixing factor $\mu$ but the fraction of nodes that lie in more than one community and fix the mixing parameter to either $0.1$ or $0.3$. The parameters are set the same way as in the non--overlapping case.


\paragraph{Seed Generation.}
Once we obtain the ground truth we have to decide on a reasonable seed node 
picking strategy. Since our premise is that seed nodes are typically well known vertices who have a lot of neighbors i. e. are structurally important, we choose to pick a fixed percentage of nodes per community, whose probability to be picked is proportional to their degree. This means nodes with high degree are more likely to be picked than others. In our experiments, we first decided to pick $5$, $10$, $15$ and $20$ percent as seed nodes. As this numbers were already quite high, we decided to improve this method so that we could use smaller values. For our improved (iterative) method we chose $2$, $4$, $6$, $8$ and $10$ percent seed nodes.

\paragraph{Random Walk.}
The coded algorithm itself is a realization of the theoretical formulations we 
established in Chapter~\ref{sec:algorithm}. Since initial test runs required a high amount of seed nodes $S$ in order to get satisfying results, we further enhanced our algorithm by implementing an iterative computation method. After one run of the regular algorithm we try to increase the amount of seed nodes per community. We do so by checking the highest affinity value $a < 1$ per vertex and include it to a set of potential new seed nodes for the respective community. After we established at most $d * \left| S \right|$ new seed nodes per community, we rerun the algorithm. The parameter $d$ sets the desired enlargement of $S$ and was set to $0.1$ in our experiments. We repeat this process for $10$ iterations.

\paragraph{Classification.}
The algorithm outputs affinity values for every vertex community combination. 
these values range between zero and one and are the sole factor whether we 
assign a vertex to a community or not. At this point the strategy for community classification is different depending on whether we deal with overlapping communities or not. In the latter case we know that a vertex is assigned to exactly one community. thus, we assign the community for which the respective vertex has the highest respective affinity value. 

In the overlapping case we apply a slightly more sophisticated strategy, since a vertex is maybe assigned to not just one but potentially all communities. First, we assume that there are one or more ``high'' affinity values which would correspond to the 
case that a vertex is in favor to be assigned to a community. On the other hand, there are one or more ``low'' values in the other case. However, we can not make any more qualified statements about the actual values of the affinities, since these numbers differ greatly depending on the structure of the graph and on the amount of communities a vertex belongs to. Therefore, we can only assume that there is some difference in high and low values. For this reason, we first sort the affinities in descending order. then we compare all subsequent pairs by computing the difference. Then we define the point where we found the largest difference as our border between low and high affinity values. Any associated community left of this border is assigned to the respective vertex, whereas any affinity value on the right hand side is considered as a low value and thus the respective community is not assigned.

\paragraph{NMI.}
After we have assigned the communities to our vertices we now want to measure 
the quality of our predictions. As Lacichinetti and Fortunato 
suggest~\cite{LF09} we determine the NMI values of the predicted community 
assignment and the ground truth. The NMI determines how similar the 
information in bits is. \texttt{This section could have a little more text, as to what NMI is}
